import difflib
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# Download required NLTK data (only needed once)
nltk.download('punkt')

def evaluate_responses(query, actual_response, llm_response):
    """
    Evaluate a single test case using BLEU score and fuzzy matching ratio.
    
    Args:
        query (str): The user query.
        actual_response (str): The expected response.
        llm_response (str): The response generated by the LLM.
        
    Returns:
        dict: A dictionary with evaluation metrics.
    """
    # Tokenize the responses for BLEU evaluation.
    actual_tokens = nltk.word_tokenize(actual_response)
    llm_tokens = nltk.word_tokenize(llm_response)
    
    # Smoothing function to handle cases with few tokens.
    smoothie = SmoothingFunction().method4
    bleu_score = sentence_bleu([actual_tokens], llm_tokens, smoothing_function=smoothie)
    
    # Compute a fuzzy matching ratio using difflib
    matcher = difflib.SequenceMatcher(None, actual_response, llm_response)
    fuzzy_ratio = matcher.ratio()
    
    return {
        "bleu_score": bleu_score,
        "fuzzy_ratio": fuzzy_ratio
    }

def main():
    # Define your test cases as a list of dictionaries.
    test_cases = [
        {
            "query": "What is the capital of France?",
            "actual_response": "Paris is the capital of France.",
            "llm_response": "Paris is the capital city of France."
        },
        {
            "query": "How many states are there in the USA?",
            "actual_response": "There are 50 states in the USA.",
            "llm_response": "USA has 50 states."
        },
        {
            "query": "Who wrote '1984'?",
            "actual_response": "George Orwell wrote '1984'.",
            "llm_response": "1984 was written by George Orwell."
        }
    ]
    
    # Evaluate each test case.
    for idx, test in enumerate(test_cases, start=1):
        query = test["query"]
        actual_response = test["actual_response"]
        llm_response = test["llm_response"]
        
        results = evaluate_responses(query, actual_response, llm_response)
        bleu = results["bleu_score"]
        fuzzy_ratio = results["fuzzy_ratio"]
        
        print(f"Test Case {idx}:")
        print(f"Query:            {query}")
        print(f"Actual Response:  {actual_response}")
        print(f"LLM Response:     {llm_response}")
        print(f"BLEU Score:       {bleu:.4f}")
        print(f"Fuzzy Ratio:      {fuzzy_ratio:.4f}")
        print("-" * 50)

if __name__ == '__main__':
    main()
